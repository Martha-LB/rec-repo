{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jax\n",
      "  Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jaxlib\n",
      "  Downloading jaxlib-0.4.30-cp39-cp39-macosx_11_0_arm64.whl (66.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 66.7 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum in /opt/anaconda3/lib/python3.9/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/anaconda3/lib/python3.9/site-packages (from jax) (1.26.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /opt/anaconda3/lib/python3.9/site-packages (from jax) (4.11.3)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from jax) (0.3.2)\n",
      "Collecting scipy>=1.9\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.3 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.6->jax) (3.7.0)\n",
      "Installing collected packages: scipy, jaxlib, jax\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "Successfully installed jax-0.4.30 jaxlib-0.4.30 scipy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Matrix U shape: (100, 10)\n",
      "Initial Matrix V shape: (10, 100)\n",
      "Epoch 1/5\n",
      "Epoch 2/5\n",
      "Epoch 3/5\n",
      "Epoch 4/5\n",
      "Epoch 5/5\n",
      "Final Matrix U shape: (100, 10)\n",
      "Final Matrix V shape: (10, 100)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Define configuration class inline\n",
    "class ConfigLf:\n",
    "    num_epochs = 5\n",
    "    batch_size_training = 32\n",
    "    batch_size_predict_with_mse = 64\n",
    "    fixed_learning_rate = 0.01\n",
    "    reg_param = 0.1\n",
    "    dyn_lr_initial = 0.01\n",
    "    num_factors = 10\n",
    "    rng_seed = 42\n",
    "    num_predictions_to_show = 5\n",
    "    num_records_predict_and_compare = 10\n",
    "    dyn_lr_decay_rate = 0.95\n",
    "    dyn_lr_steps = 10\n",
    "\n",
    "config = ConfigLf()\n",
    "\n",
    "# Mock data loading function\n",
    "def load_data_and_init_factors(config):\n",
    "    num_users = 100  # Set the number of users\n",
    "    num_items = 100  # Set the number of items\n",
    "    rng_key_factors = jax.random.PRNGKey(config.rng_seed)\n",
    "    \n",
    "    # Initialize user and item factor matrices\n",
    "    matrix_u = jax.random.normal(rng_key_factors, (num_users, config.num_factors))\n",
    "    matrix_v = jax.random.normal(rng_key_factors, (config.num_factors, num_items))\n",
    "    \n",
    "    # Mock ratings tensor (for testing purposes)\n",
    "    ratings_tf = jax.random.randint(rng_key_factors, shape=(num_users, num_items), minval=1, maxval=10)\n",
    "    \n",
    "    return ratings_tf, matrix_u, matrix_v, num_users, num_items\n",
    "\n",
    "# Implement a placeholder for data splitting if necessary\n",
    "def split_train_valid_test_tf(ratings_tf, config):\n",
    "    # Split the ratings_tf into train, validation, and test datasets\n",
    "    # For simplicity, we'll return the same mock dataset for all three\n",
    "    train_ds = ratings_tf\n",
    "    valid_ds = ratings_tf\n",
    "    test_ds = ratings_tf\n",
    "    return train_ds, valid_ds, test_ds\n",
    "\n",
    "# Implement your SGD functions and helper functions\n",
    "def uv_factorization_reg(mat_u, mat_v, train_ds, config):\n",
    "    \"\"\" Matrix factorization using SGD with regularization \"\"\"\n",
    "    @jax.jit  # Just-in-time compilation for efficiency\n",
    "    def update_uv(mat_u, mat_v, record, lr, reg_param):\n",
    "        i, j, rating = record[0], record[1], record[2]  # Unpack the record\n",
    "        error = rating - jnp.dot(mat_u[i], mat_v[:, j])\n",
    "        # Update rules with regularization\n",
    "        mat_u = mat_u.at[i].add(lr * (error * mat_v[:, j] - reg_param * mat_u[i]))\n",
    "        mat_v = mat_v.at[:, j].add(lr * (error * mat_u[i] - reg_param * mat_v[:, j]))\n",
    "        return mat_u, mat_v\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        for record in jax.device_get(train_ds):\n",
    "            mat_u, mat_v = update_uv(mat_u, mat_v, record, config.fixed_learning_rate, config.reg_param)\n",
    "\n",
    "    return mat_u, mat_v\n",
    "\n",
    "# Test the functions\n",
    "if __name__ == '__main__':\n",
    "    # Load data and initialize factors\n",
    "    ratings_tf, matrix_u, matrix_v, num_users, num_items = load_data_and_init_factors(config)\n",
    "    train_ds, valid_ds, test_ds = split_train_valid_test_tf(ratings_tf, config)\n",
    "\n",
    "    # Show initial metrics before optimization\n",
    "    print(f\"Initial Matrix U shape: {matrix_u.shape}\")\n",
    "    print(f\"Initial Matrix V shape: {matrix_v.shape}\")\n",
    "\n",
    "    # Run the SGD with regularization\n",
    "    matrix_u, matrix_v = uv_factorization_reg(matrix_u, matrix_v, train_ds, config)\n",
    "\n",
    "    # Show final metrics after optimization\n",
    "    print(f\"Final Matrix U shape: {matrix_u.shape}\")\n",
    "    print(f\"Final Matrix V shape: {matrix_v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artur Andrzejak, October 2024\n",
    "# Algorithms for latent factor models\n",
    "\n",
    "# Limit size of GPU memory pre-allocated by jax\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import dataclasses\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def init_latent_factors(num_users, num_items, num_factors, rng_key):\n",
    "    \"\"\" Initialize latent factors for users and items\n",
    "    \"\"\"\n",
    "    key_u, key_v = jax.random.split(rng_key)\n",
    "    matrix_u = jax.random.normal(key_u, (num_items, num_factors))\n",
    "    matrix_v = jax.random.normal(key_v, (num_factors, num_users))\n",
    "    return matrix_u, matrix_v\n",
    "\n",
    "\n",
    "def load_data_and_init_factors(config):\n",
    "    # load the dataset using TensorFlow Datasets\n",
    "    import data_util as data\n",
    "\n",
    "    ratings_tf, user_ids_voc, movie_ids_voc = data.load_movielens_tf(config)\n",
    "    num_users = len(user_ids_voc.get_vocabulary())\n",
    "    num_items = len(movie_ids_voc.get_vocabulary())\n",
    "    rng_key_factors, rng_key_r = jax.random.split(jax.random.PRNGKey(config.rng_seed))\n",
    "    matrix_u, matrix_v = init_latent_factors(num_users, num_items, config.num_factors, rng_key_factors)\n",
    "    return ratings_tf, matrix_u, matrix_v, num_users, num_items\n",
    "\n",
    "\n",
    "def predict_and_compare(mat_u, mat_v, train_ds, config):\n",
    "    \"\"\" Predict ratings for the test dataset, compare to target ratings\n",
    "        Returns a list of tuples with (predicted, target) ratings\"\"\"\n",
    "\n",
    "    predictions_and_targets = []\n",
    "    # Only batch_size=1 is supported for now\n",
    "\n",
    "    for idx, record in enumerate(tfds.as_numpy(train_ds.batch(1))):\n",
    "        # Batch sizes > 1 compute too many predictions (all pairs of users and items)\n",
    "        # i, j, rating = record[\"user_id\"], record[\"movie_id\"], record[\"user_rating\"]\n",
    "        # rating_pred = jnp.dot(mat_u[i, :], mat_v[:, j])\n",
    "        i, j, rating = record[\"movie_id\"][0], record[\"user_id\"][0], float(record[\"user_rating\"][0])\n",
    "        rating_pred = float(jnp.dot(mat_u[i, :], mat_v[:, j]))\n",
    "        predictions_and_targets.append((rating_pred, rating))\n",
    "        if idx >= config.num_records_predict_and_compare:\n",
    "            break\n",
    "    return predictions_and_targets\n",
    "\n",
    "\n",
    "def mse_loss_all_batches(mat_u, mat_v, dataset, batch_size):\n",
    "    \"\"\" Compute mse per batch using vectorized operations\n",
    "        Returns a list of mse values for all batches as floats\n",
    "    \"\"\"\n",
    "    mse_all_batches = []\n",
    "    for record in tfds.as_numpy(dataset.batch(batch_size)):\n",
    "        mse = mse_loss_one_batch(mat_u, mat_v, record)\n",
    "        mse_all_batches.append(mse)\n",
    "    # convert list of arrays to list of floats\n",
    "    mse_all_batches = list(map(float, mse_all_batches))\n",
    "    return mse_all_batches\n",
    "\n",
    "\n",
    "@jax.jit  # Comment out for single-step debugging\n",
    "def mse_loss_one_batch(mat_u, mat_v, record):\n",
    "    \"\"\"This colab experiment motivates the implementation:\n",
    "    https://colab.research.google.com/drive/1c0LpSndbTJaHVoLTatQCbGhlsWbpgvYh?usp&#x3D;sharing=\n",
    "    \"\"\"\n",
    "    rows, columns, ratings = record[\"movie_id\"], record[\"user_id\"], record[\"user_rating\"]\n",
    "    estimator = -(mat_u @ mat_v)[(rows, columns)]\n",
    "    square_err = jnp.square(estimator + ratings)\n",
    "    mse = jnp.mean(square_err)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def uv_factorization_dense_um(mat_u, mat_v, mat_um, num_epochs=1, learning_rate=0.01, reg_param=0.1):\n",
    "    \"\"\" Matrix factorization using stochastic gradient descent (SGD) for a dense utility matrix\n",
    "        Terribly slow implementation, just for illustration purposes\n",
    "    Args:\n",
    "        mat_u: user factor matrix\n",
    "        mat_v: item factor matrix\n",
    "        mat_um: utility matrix\n",
    "        num_epochs: number of iterations\n",
    "        learning_rate: step size for gradient descent\n",
    "        reg_param: regularization parameter\n",
    "    Returns:\n",
    "        U: learned user matrix\n",
    "        V: learned item matrix\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        # Will be printed during tracing, not execution!\n",
    "        print(f\"In uv_factorization_dense_um, starting epoch {epoch}\")\n",
    "        for i in range(mat_um.shape[0]):\n",
    "            for j in range(mat_um.shape[1]):\n",
    "                eij = mat_um[i, j] - jnp.dot(mat_u[i, :], mat_v[:, j])\n",
    "                # Will be printed during tracing, not execution!\n",
    "                # print (f\"Current i,j, eij are: {i}, {j}, {eij}\")\n",
    "                for k in range(mat_u.shape[1]):\n",
    "                    mat_u = mat_u.at[i, k].add(learning_rate * (eij * mat_v[k, j] - reg_param * mat_u[i, k]))\n",
    "                    mat_v = mat_v.at[k, j].add(learning_rate * (eij * mat_u[i, k] - reg_param * mat_v[k, j]))\n",
    "    return mat_u, mat_v\n",
    "\n",
    "\n",
    "# jit-compile the function, pointing out the static arguments\n",
    "uv_factorization_dense_um = jax.jit(uv_factorization_dense_um,\n",
    "                                    static_argnames=(\"num_epochs\", \"learning_rate\", \"reg_param\"))\n",
    "\n",
    "\n",
    "def uv_factorization_tf_slow(mat_u, mat_v, train_ds, config):\n",
    "    \"\"\" Matrix factorization using stochastic gradient descent (SGD) for sparse (and batched) utility matrix\n",
    "        A terribly slow implementation, just for illustration purposes\n",
    "    \"\"\"\n",
    "\n",
    "    def update_uv_for_record(config, mat_u, mat_v, record):\n",
    "        lr, rp = 0.1, config.reg_param\n",
    "        i, j, rating = record[\"movie_id\"], record[\"user_id\"], record[\"user_rating\"]\n",
    "        # Convert np arrays to scalars - only ok for batch size 1!\n",
    "        i, j, rating = i[0], j[0], rating[0]\n",
    "        abs_error_ij = rating - jnp.dot(mat_u[i, :], mat_v[:, j])\n",
    "        for k in range(mat_u.shape[1]):\n",
    "            mat_u = mat_u.at[i, k].add(lr * (abs_error_ij * mat_v[k, j] - rp * mat_u[i, k]))\n",
    "            mat_v = mat_v.at[k, j].add(lr * (abs_error_ij * mat_u[i, k] - rp * mat_v[k, j]))\n",
    "        return mat_u, mat_v\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"In uv_factorization_tf_slow, starting epoch {epoch}\")\n",
    "        for record in tfds.as_numpy(train_ds.batch(config.batch_size_training).prefetch(config.batch_size_training)):\n",
    "            mat_u, mat_v = update_uv_for_record(config, mat_u, mat_v, record)\n",
    "    return mat_u, mat_v\n",
    "\n",
    "\n",
    "def uv_factorization_vec_no_reg(mat_u, mat_v, train_ds, valid_ds, config):\n",
    "    \"\"\" Matrix factorization using SGD without regularization\n",
    "        Fast vectorized implementation using JAX\n",
    "    \"\"\"\n",
    "\n",
    "    @jax.jit  # Comment out for single-step debugging\n",
    "    def update_uv(mat_u, mat_v, record, lr):\n",
    "        loss_value, grad = jax.value_and_grad(mse_loss_one_batch, argnums=[0, 1])(mat_u, mat_v, record)\n",
    "        mat_u = mat_u - lr * grad[0]\n",
    "        mat_v = mat_v - lr * grad[1]\n",
    "        return mat_u, mat_v, loss_value\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        lr = config.fixed_learning_rate if config.fixed_learning_rate is not None \\\n",
    "            else config.dyn_lr_initial * (config.dyn_lr_decay_rate ** (epoch / config.dyn_lr_steps))\n",
    "        print(f\"In uv_factorization_vec_no_reg, starting epoch {epoch} with lr={lr:.6f}\")\n",
    "        train_loss = []\n",
    "        for record in tfds.as_numpy(train_ds.batch(config.batch_size_training)):\n",
    "            mat_u, mat_v, loss = update_uv(mat_u, mat_v, record, lr)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "        train_loss_mean = jnp.mean(jnp.array(train_loss))\n",
    "        # Compute loss on the validation set\n",
    "        valid_loss = mse_loss_all_batches(mat_u, mat_v, valid_ds, config.batch_size_predict_with_mse)\n",
    "        valid_loss_mean = jnp.mean(jnp.array(valid_loss))\n",
    "        print(\n",
    "            f\"Epoch {epoch} finished, ave training loss: {train_loss_mean:.6f}, ave validation loss: {valid_loss_mean:.6f}\")\n",
    "    return mat_u, mat_v\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Flags:\n",
    "    evaluate_uv_factorization_dense_um = False\n",
    "    evaluate_uv_factorization_tf_slow = False\n",
    "    evaluate_uv_factorization_vec_no_reg = True\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "if __name__ == '__main__':\n",
    "    from config import ConfigLf as config\n",
    "    import data_util as data\n",
    "\n",
    "    if Flags.evaluate_uv_factorization_dense_um:\n",
    "        num_users = 10\n",
    "        num_items = 10\n",
    "        num_factors = 5\n",
    "        num_epochs = 2\n",
    "        seed = 42\n",
    "\n",
    "        rng_key_factors, rng_key_r = jax.random.split(jax.random.PRNGKey(seed))\n",
    "        matrix_u, matrix_v = init_latent_factors(num_users, num_items, num_factors, rng_key_factors)\n",
    "        ratings = jax.random.randint(rng_key_r, shape=(num_users, num_items), minval=1, maxval=10)\n",
    "        matrix_u, matrix_v = uv_factorization_dense_um(matrix_u, matrix_v, ratings, num_epochs=num_epochs)\n",
    "        print(\"Results of uv_factorization_dense_um\")\n",
    "        print(matrix_u)\n",
    "        print(matrix_v)\n",
    "\n",
    "    if Flags.evaluate_uv_factorization_tf_slow:\n",
    "        ratings_tf, matrix_u, matrix_v, num_users, num_items = load_data_and_init_factors(config)\n",
    "        train_ds, valid_ds, test_ds = data.split_train_valid_test_tf(ratings_tf, config)\n",
    "\n",
    "        # Dummy predictions, with random factors\n",
    "        predictions_and_targets = predict_and_compare(matrix_u, matrix_v, test_ds, config)\n",
    "        print(\"Prediction examples (pred, target) before optimization\")\n",
    "        print(predictions_and_targets[:config.num_predictions_to_show])\n",
    "\n",
    "        # Optimize the factors and show the predictions\n",
    "        matrix_u, matrix_v = uv_factorization_tf_slow(matrix_u, matrix_v, train_ds, config)\n",
    "        predictions_and_targets = predict_and_compare(matrix_u, matrix_v, test_ds, config)\n",
    "        print(\"Results of uv_factorization_tf_slow\")\n",
    "        print(\"Prediction examples (pred, target) after optimization\")\n",
    "        print(predictions_and_targets[:config.num_predictions_to_show])\n",
    "\n",
    "    if Flags.evaluate_uv_factorization_vec_no_reg:\n",
    "        ratings_tf, matrix_u, matrix_v, num_users, num_items = load_data_and_init_factors(config)\n",
    "        train_ds, valid_ds, test_ds = data.split_train_valid_test_tf(ratings_tf, config)\n",
    "\n",
    "\n",
    "        def show_metrics_and_examples(message, matrix_u, matrix_v):\n",
    "            print(message)\n",
    "            mse_all_batches = mse_loss_all_batches(matrix_u, matrix_v, test_ds, config.batch_size_predict_with_mse)\n",
    "            print(\"MSE examples from predict_with_mse on test_ds\")\n",
    "            print(mse_all_batches[:config.num_predictions_to_show])\n",
    "            print(\"Prediction examples (pred, target)\")\n",
    "            predictions_and_targets = predict_and_compare(matrix_u, matrix_v, test_ds, config)\n",
    "            print(predictions_and_targets[:config.num_predictions_to_show])\n",
    "\n",
    "\n",
    "        show_metrics_and_examples(\"====== Before optimization =====\", matrix_u, matrix_v)\n",
    "\n",
    "        # Optimize the factors fast\n",
    "        matrix_u, matrix_v = uv_factorization_vec_no_reg(matrix_u, matrix_v, train_ds, valid_ds, config)\n",
    "\n",
    "        show_metrics_and_examples(\"====== After optimization =====\", matrix_u, matrix_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task a\n",
    "\n",
    "def uv_factorization_reg(mat_u, mat_v, train_ds, valid_ds, config):\n",
    "    \"\"\" Matrix factorization using SGD with regularization\n",
    "        Fast vectorized implementation using JAX\n",
    "    \"\"\"\n",
    "    @jax.jit  # JIT compile for performance\n",
    "    def update_uv_with_reg(mat_u, mat_v, record, lr, reg_param):\n",
    "        # Compute loss and gradient with respect to U and V\n",
    "        loss_value, grad = jax.value_and_grad(mse_loss_one_batch, argnums=[0, 1])(mat_u, mat_v, record)\n",
    "        # Apply regularization to gradients\n",
    "        grad_u = grad[0] + reg_param * mat_u\n",
    "        grad_v = grad[1] + reg_param * mat_v\n",
    "        # Update matrices with regularized gradients\n",
    "        mat_u -= lr * grad_u\n",
    "        mat_v -= lr * grad_v\n",
    "        return mat_u, mat_v, loss_value\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(config.num_epochs):\n",
    "        lr = config.fixed_learning_rate if config.fixed_learning_rate is not None else config.dyn_lr_initial\n",
    "        reg_param = config.reg_param\n",
    "        print(f\"In uv_factorization_reg, starting epoch {epoch} with lr={lr:.6f} and reg_param={reg_param:.6f}\")\n",
    "\n",
    "        train_loss = []\n",
    "        for record in tfds.as_numpy(train_ds.batch(config.batch_size_training)):\n",
    "            mat_u, mat_v, loss = update_uv_with_reg(mat_u, mat_v, record, lr, reg_param)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "        # Compute mean loss for training\n",
    "        train_loss_mean = jnp.mean(jnp.array(train_loss))\n",
    "\n",
    "        # Compute mean loss for validation\n",
    "        valid_loss = mse_loss_all_batches(mat_u, mat_v, valid_ds, config.batch_size_predict_with_mse)\n",
    "        valid_loss_mean = jnp.mean(jnp.array(valid_loss))\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss_mean:.6f}, Validation Loss = {valid_loss_mean:.6f}\")\n",
    "\n",
    "    return mat_u, mat_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matrices and datasets\n",
    "ratings_tf, matrix_u, matrix_v, num_users, num_items = load_data_and_init_factors(config)\n",
    "train_ds, valid_ds, test_ds = data.split_train_valid_test_tf(ratings_tf, config)\n",
    "\n",
    "# Show initial metrics before optimization\n",
    "show_metrics_and_examples(\"====== Before optimization (Regularized) =====\", matrix_u, matrix_v)\n",
    "\n",
    "# Run the regularized matrix factorization\n",
    "matrix_u, matrix_v = uv_factorization_reg(matrix_u, matrix_v, train_ds, valid_ds, config)\n",
    "\n",
    "# Show metrics after optimization\n",
    "show_metrics_and_examples(\"====== After optimization (Regularized) =====\", matrix_u, matrix_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task b\n",
    "\n",
    "import itertools\n",
    "\n",
    "def grid_search_uv_factorization(train_ds, valid_ds, matrix_u_init, matrix_v_init, config):\n",
    "    \"\"\" Perform grid search for hyperparameters fixed_learning_rate and reg_param \"\"\"\n",
    "    # Define grid for hyperparameters\n",
    "    learning_rates = jnp.linspace(0.001, 0.1, 5)  # Sample values for learning rates\n",
    "    reg_params = jnp.linspace(0.01, 0.1, 5)       # Sample values for regularization params\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    best_hyperparams = None\n",
    "    best_matrix_u, best_matrix_v = None, None\n",
    "\n",
    "    for lr, reg in itertools.product(learning_rates, reg_params):\n",
    "        # Reset U and V matrices for each grid search trial\n",
    "        matrix_u, matrix_v = matrix_u_init.copy(), matrix_v_init.copy()\n",
    "\n",
    "        # Update config with current hyperparameters\n",
    "        config.fixed_learning_rate = lr\n",
    "        config.reg_param = reg\n",
    "\n",
    "        # Run training with regularization\n",
    "        print(f\"Testing lr={lr:.6f}, reg={reg:.6f}\")\n",
    "        matrix_u, matrix_v = uv_factorization_reg(matrix_u, matrix_v, train_ds, valid_ds, config)\n",
    "\n",
    "        # Compute validation loss\n",
    "        valid_loss = mse_loss_all_batches(matrix_u, matrix_v, valid_ds, config.batch_size_predict_with_mse)\n",
    "        valid_loss_mean = jnp.mean(jnp.array(valid_loss))\n",
    "\n",
    "        # Update best model if validation loss improves\n",
    "        if valid_loss_mean < best_valid_loss:\n",
    "            best_valid_loss = valid_loss_mean\n",
    "            best_hyperparams = (lr, reg)\n",
    "            best_matrix_u, best_matrix_v = matrix_u, matrix_v\n",
    "\n",
    "    print(f\"Best hyperparameters: lr={best_hyperparams[0]}, reg={best_hyperparams[1]} with validation loss {best_valid_loss:.6f}\")\n",
    "    return best_matrix_u, best_matrix_v, best_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search to find optimal hyperparameters\n",
    "best_matrix_u, best_matrix_v, best_hyperparams = grid_search_uv_factorization(train_ds, valid_ds, matrix_u_init, matrix_v_init, config)\n",
    "\n",
    "# Display results for the best regularized model\n",
    "show_metrics_and_examples(\"====== After optimization with best regularized model =====\", best_matrix_u, best_matrix_v)\n",
    "\n",
    "# Run the non-regularized model for comparison\n",
    "matrix_u, matrix_v = uv_factorization_vec_no_reg(matrix_u_init, matrix_v_init, train_ds, valid_ds, config)\n",
    "show_metrics_and_examples(\"====== After optimization with non-regularized model =====\", matrix_u, matrix_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
